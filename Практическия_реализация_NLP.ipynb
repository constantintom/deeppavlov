{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MDzsh_rqrGD"
   },
   "source": [
    "# Практическия реализация NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3MQ-j3-XzqM"
   },
   "source": [
    "In this assignment you will perform sentiment analysis of the IMDBs reviews by using RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZ3tq2efXzqN",
    "outputId": "e6266247-0ef6-4020-8fb4-14cb2c93d02f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torch==1.6.0\n",
      "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 748.8 MB 17 kB/s \n",
      "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.21.6)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.0+cu113\n",
      "    Uninstalling torch-1.12.0+cu113:\n",
      "      Successfully uninstalled torch-1.12.0+cu113\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.6.0 which is incompatible.\n",
      "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.6.0 which is incompatible.\n",
      "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.6.0 which is incompatible.\n",
      "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.6.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.6.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torchtext==0.7\n",
      "  Downloading torchtext-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 31.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (4.64.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (1.21.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (2.23.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 49.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (1.6.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7) (2022.6.15)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.7) (0.16.0)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.13.0\n",
      "    Uninstalling torchtext-0.13.0:\n",
      "      Successfully uninstalled torchtext-0.13.0\n",
      "Successfully installed sentencepiece-0.1.96 torchtext-0.7.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.6.0\n",
    "!pip install torchtext==0.7\n",
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFTDWATQ1BVN",
    "outputId": "94718369-2731-49bb-a8cc-959395bd1ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bfusPGesXzqP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchtext import datasets\n",
    "\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axvxFxD0XzqP"
   },
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uFRWg-SDXzqQ"
   },
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, lower=True)\n",
    "LABEL = LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-KjrlhxoXzqQ",
    "outputId": "49085ea2-a6fa-4131-fa62-3973a61d4ea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 9.82MB/s]\n"
     ]
    }
   ],
   "source": [
    "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
    "trn, vld = train.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kfGodmCXzqR",
    "outputId": "2c4d4169-cf39-426b-89c3-b292e4c5ae78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 35.7 ms, total: 1.22 s\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nGz60xZeXzqS"
   },
   "outputs": [],
   "source": [
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBZXFxWQXzqT",
    "outputId": "396e8555-8244-4570-b92f-bbb74cd36ed8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 225754),\n",
       " ('a', 112289),\n",
       " ('and', 111408),\n",
       " ('of', 101336),\n",
       " ('to', 93885),\n",
       " ('is', 73019),\n",
       " ('in', 63516),\n",
       " ('i', 49152),\n",
       " ('this', 48709),\n",
       " ('that', 46310)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcpRl46sXzqT"
   },
   "source": [
    "### Creating the Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1UXNIABXzqU"
   },
   "source": [
    "During training, we'll be using a special kind of Iterator, the **BucketIterator**. \n",
    "\n",
    "All the neural networks require to have inputs of the same shape and size. So the data saples should be padded to the same length before gathering them into batches:\n",
    "\n",
    "e.g.\n",
    "\\[ \n",
    "\\[3, 15, 2, 7\\],\n",
    "\\[4, 1\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] -> \\[ \n",
    "\\[3, 15, 2, 7, **0**\\],\n",
    "\\[4, 1, **0**, **0**, **0**\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] \n",
    "\n",
    "If the sequences of one batch differ greatly in length, the padding will consume a lot of wasteful memory and time. The BucketIterator groups sequences of similar lengths together for each batch to minimize padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQBuXYjnbIZX"
   },
   "source": [
    "The **BucketIterator** usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gDZcwvzXXzqU"
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "        (trn, vld, tst),\n",
    "        batch_sizes=(64, 64, 64),\n",
    "        sort=True,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=False,\n",
    "        device='cuda',\n",
    "        repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eae_lu0GXzqV"
   },
   "source": [
    "Let's take a look at the output of the BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjmCSB9aXzqV",
    "outputId": "3777a80c-c071-4c3a-971f-47f3223831f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   10,     9,  1245,  ...,   117,     9,    10],\n",
       "        [   20,   509,   140,  ...,   208,   376,    20],\n",
       "        [    7,   875,  2186,  ...,   117,     2,     7],\n",
       "        ...,\n",
       "        [    1,     1,     1,  ...,   769,   226, 12889],\n",
       "        [    1,     1,     1,  ...,    12,   508,    13],\n",
       "        [    1,     1,     1,  ..., 10845,   112,   728]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hm9IPdAXzqV"
   },
   "source": [
    "The batch contains all the fields we passed to the Dataset object that can be accessed as attributes with the corresponding names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-xNoF1S0XzqW",
    "outputId": "006a7011-9c53-4dc3-a93c-4255920b40fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'fields', 'input_fields', 'target_fields', 'text', 'label'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XJ8mmEZXzqW"
   },
   "source": [
    "### Define the RNN-based text classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3MeuXZTXzqW"
   },
   "source": [
    "Let's start with the simple architecture. Implement the model according to the scheme below.  \n",
    "![alt text](https://miro.medium.com/max/1396/1*v-tLYQCsni550A-hznS0mw.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "979yo1iSXzqW"
   },
   "outputs": [],
   "source": [
    "class RNNBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim, vocab_dim):\n",
    "        super(RNNBaseline, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vocab_dim = vocab_dim        \n",
    "        layer_dim = 4\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_dim, emb_dim)\n",
    "        \n",
    "        self.encoder = nn.GRU(emb_dim, hidden_dim, layer_dim)\n",
    "        \n",
    "        self.decoder = nn.Linear(2*hidden_dim, 64)\n",
    "            \n",
    "    def forward(self, seq):\n",
    "        \n",
    "        emb = self.embedding(seq)\n",
    "        \n",
    "        outputs, _ = self.encoder(emb)\n",
    "        \n",
    "        encoding = torch.cat((outputs[0], outputs[1]), -1)\n",
    "       \n",
    "       \n",
    "        preds = self.decoder(encoding)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytcKa0sLXzqX",
    "outputId": "1b1e4a46-3e2d-4124-ee67-5ddfe409d2d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (embedding): Embedding(201052, 200)\n",
       "  (encoder): GRU(200, 300, num_layers=4)\n",
       "  (decoder): Linear(in_features=600, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_sz = 200\n",
    "nh = 300\n",
    "v_size = len(TEXT.vocab)\n",
    "model = RNNBaseline(nh, em_sz, v_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvctmT6MXzqX"
   },
   "source": [
    "*If* you're using GPU, remember to call model.cuda() to move your model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BEmLspl-XzqX",
    "outputId": "d42c4ca0-a6a6-46ad-98f2-167f22d14975"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (embedding): Embedding(201052, 200)\n",
       "  (encoder): GRU(200, 300, num_layers=4)\n",
       "  (decoder): Linear(in_features=600, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wttBF3JLXzqY"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4HdCt7gXzqY"
   },
   "source": [
    "Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QruMA8qOXzqY"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiQn6YO8XzqZ"
   },
   "source": [
    "Set the number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFRXDxatXzqZ"
   },
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwJGSsAJ3Y0F"
   },
   "outputs": [],
   "source": [
    "def save_model(model, iter):\n",
    "  path = f'/content/drive/My Drive/Model/_iter_{iter}'\n",
    "  print(f'Saving {iter} model...')\n",
    "  torch.save(model, path)\n",
    "  print(f'{iter} saved successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWdzskyGkJLw"
   },
   "source": [
    "Finally, run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3Aip0CkXzqZ",
    "outputId": "7f7a4e81-c134-492a-e3f6-1c9d80e0cc43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.01099149864401136, Validation Loss: 0.010946952231725057\n",
      "Saving 1 model...\n",
      "1 saved successfully.\n",
      "Epoch: 2, Training Loss: 0.01083477886063712, Validation Loss: 0.011152411619822184\n",
      "Saving 2 model...\n",
      "2 saved successfully.\n",
      "Epoch: 3, Training Loss: 0.010438943416731699, Validation Loss: 0.011444394063949585\n",
      "Saving 3 model...\n",
      "3 saved successfully.\n",
      "Epoch: 4, Training Loss: 0.009872654577663967, Validation Loss: 0.011823897043863931\n",
      "Saving 4 model...\n",
      "4 saved successfully.\n",
      "Epoch: 5, Training Loss: 0.009281155111108507, Validation Loss: 0.012725892392794291\n",
      "Saving 5 model...\n",
      "5 saved successfully.\n",
      "Epoch: 6, Training Loss: 0.008788975258384433, Validation Loss: 0.013775794672966003\n",
      "Saving 6 model...\n",
      "6 saved successfully.\n",
      "Epoch: 7, Training Loss: 0.008367549686772483, Validation Loss: 0.013699033331871033\n",
      "Saving 7 model...\n",
      "7 saved successfully.\n",
      "Epoch: 8, Training Loss: 0.007973204700435912, Validation Loss: 0.014367634884516398\n",
      "Saving 8 model...\n",
      "8 saved successfully.\n",
      "Epoch: 9, Training Loss: 0.007650689155714852, Validation Loss: 0.015195351346333822\n",
      "Saving 9 model...\n",
      "9 saved successfully.\n",
      "Epoch: 10, Training Loss: 0.00748739242042814, Validation Loss: 0.014771487522125244\n",
      "Saving 10 model...\n",
      "10 saved successfully.\n",
      "CPU times: user 4min 54s, sys: 9.65 s, total: 5min 4s\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for batch in train_iter: \n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        preds = model(x)   \n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        preds = model(x) \n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(vld)\n",
    "    print(f'Epoch: {epoch}, Training Loss: {epoch_loss}, Validation Loss: {val_loss}')\n",
    "    save_model(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5ZIeZMVOdyl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8EaZxsjO8FL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_Vw14WxXzqZ"
   },
   "source": [
    "### Calculate performance of the trained model (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "291eQyyHXzqa",
    "outputId": "c3b36342-d4ab-4212-a755-64f5ab51b01e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Accuracy: 0.5431345907928389, Precision: 0.5628120546200523, Recall: 0.30321270477920037, F1: 0.37626781544729987\n",
      "Epoch: 2, Accuracy: 0.5431345907928389, Precision: 0.5628120546200523, Recall: 0.30321270477920037, F1: 0.37626781544729987\n",
      "Epoch: 3, Accuracy: 0.5431345907928389, Precision: 0.5628120546200523, Recall: 0.30321270477920037, F1: 0.37626781544729987\n",
      "Epoch: 4, Accuracy: 0.5431345907928389, Precision: 0.5628120546200523, Recall: 0.30321270477920037, F1: 0.37626781544729987\n",
      "Epoch: 5, Accuracy: 0.5431345907928389, Precision: 0.5628120546200523, Recall: 0.30321270477920037, F1: 0.37626781544729987\n",
      "Epoch: 6, Accuracy: 0.5431345907928389, Precision: 0.5628120546200523, Recall: 0.30321270477920037, F1: 0.37626781544729987\n",
      "Epoch: 7, Accuracy: 0.5431345907928389, Precision: 0.5628120546200523, Recall: 0.30321270477920037, F1: 0.37626781544729987\n",
      "Epoch: 8, Accuracy: 0.5431345907928389, Precision: 0.5628120546200523, Recall: 0.30321270477920037, F1: 0.37626781544729987\n",
      "Epoch: 9, Accuracy: 0.5431345907928389, Precision: 0.5628120546200523, Recall: 0.30321270477920037, F1: 0.37626781544729987\n",
      "Epoch: 10, Accuracy: 0.5431345907928389, Precision: 0.5628120546200523, Recall: 0.30321270477920037, F1: 0.37626781544729987\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "  acc = 0.0\n",
    "  pre = 0.0\n",
    "  rec = 0.0\n",
    "  f1 = 0.0\n",
    "  model.eval()\n",
    "  for batch in test_iter:\n",
    "    x = batch.text\n",
    "    y = batch.label\n",
    "    l = y.detach().cpu().numpy()\n",
    "    preds = model(x).detach().cpu().numpy().argmax(axis=1)\n",
    "    acc += accuracy_score(l, preds)\n",
    "    pre += precision_score(l, preds)\n",
    "    rec += recall_score(l, preds)\n",
    "    f1 += f1_score(l, preds)\n",
    "    i = len(test_iter)\n",
    "     \n",
    "  print(f'Epoch: {epoch}, Accuracy: {acc/i}, Precision: {pre/i}, Recall: {rec/i}, F1: {f1/i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTs5KZBBXzqa",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Report the calculated performance below\n",
    "\n",
    "#### Accuracy: 0.543\n",
    "#### Precision: 0.563\n",
    "#### Recall: 0.303\n",
    "#### F1: 0.376"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSJlVgalXzqa"
   },
   "source": [
    "### Experiments\n",
    "\n",
    "Feel free to experiment with the model to improve performance scores. You can find advices [here](https://arxiv.org/abs/1801.06146). \n",
    "\n",
    "Below describe, please, \n",
    " - your improvements and challenges you faced\n",
    " - provide your experiments' implementation details\n",
    " - explain your choice of architecture/training method/regularization techniques etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He1vAfd1Xzqa"
   },
   "source": [
    "### 1. Проведённые исследования по изменению алгоритма оптимизации показал, что наилучшие результаты по сравнению с базовой моделью Adam были получены c использованием алгоритма RMSprop и скоростью обучения 0.001 и 0.0001.\n",
    "* Эксперимент по подбору других гиперпараметров и моделированием функции потерь требует большего времени и, к сожалению, на Google Colab не хватило времени на прогон по всем параметрам. Лучших результатов можно добиться на стационарном \"железе\".\n",
    "* Также желательно использовать более новые библиотеки. В частности torch.text не поддерживается в последних версиях pytorch.\n",
    "### 2. Доработана базовая программа обучения модели. В циклах осуществлен перебор и обучение с нуля для каждого варианта.\n",
    "### 3. Изменение функции оптимизации и ее гиперпараметров является одним из ключевых способов ручного управления процессом улучшения модели.\n",
    "* Также необходимо провести эксперименты по подбору функции потерь, может быть попытаться группировать потери с регулиризацией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDzJq_5etCn3"
   },
   "source": [
    "#### Попробуем поэкспериментировать с изменением алгоритма оптимизации Adam, RMSprop, SGD и гиперпараметром  (скорость обучения lr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "130Z80d3CcDG"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam, RMSprop, SGD\n",
    "loss_func =  torch.nn.CrossEntropyLoss()\n",
    "optim_dict = {'Adam': Adam, 'RMSprop': RMSprop, 'SGD': SGD}\n",
    "    \n",
    "\n",
    "# loss_dict = {'CELoss': torch.nn.CrossEntropyLoss(), \n",
    "            #  'BCELoss': torch.nn.BCELoss(), \n",
    "            #  'BCEWLoss': torch.nn.BCEWithLogitsLoss()}\n",
    "# eps_dict = {'1e-8': 1e-8, '0.1': 0.1, '1.0': 1.0}\n",
    "lr_dict = {'0.001': 0.001,  '0.01': 0.01, '0.0001': 0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgMsjQssD7ck"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lbOm98LDxYv"
   },
   "outputs": [],
   "source": [
    "def save_model2(model, iter, loss_key, eps_key, lr_key):\n",
    "  path = f'/content/drive/My Drive/Model/_iter_{iter}' + loss_key+lr_key +eps_key\n",
    "  print(f'Saving {iter} model...')\n",
    "  torch.save(model, path)\n",
    "  print(f'{iter} saved successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ekd2YvKGvAar"
   },
   "outputs": [],
   "source": [
    "epochs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0fhC4480oYLP"
   },
   "outputs": [],
   "source": [
    "# def train_model(model, epochs, loss_func, opt, loss_key, eps_key, lr_key):\n",
    "def train_model(model, epochs, loss_func, opt):\n",
    "  %%time\n",
    "  for epoch in range(1, epochs + 1):\n",
    "      running_loss = 0.0\n",
    "      running_corrects = 0\n",
    "      model.train() \n",
    "      for batch in train_iter: \n",
    "          \n",
    "          x = batch.text\n",
    "          y = batch.label\n",
    "          \n",
    "          opt.zero_grad()\n",
    "          preds = model(x)   \n",
    "          loss = loss_func(preds, y)\n",
    "          loss.backward()\n",
    "          opt.step()\n",
    "          running_loss += loss.item()\n",
    "\n",
    "      epoch_loss = running_loss / len(trn)\n",
    "      \n",
    "      val_loss = 0.0\n",
    "      model.eval()\n",
    "      for batch in val_iter:\n",
    "          \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        preds = model(x) \n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item()\n",
    "          \n",
    "      val_loss /= len(vld)\n",
    "      print(f'Epoch: {epoch}, Training Loss: {epoch_loss}, Validation Loss: {val_loss}')\n",
    "      # save_model2(model, epoch, loss_key, eps_key, lr_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxmlwugJwZV-",
    "outputId": "c2dd9d2f-8e59-42fe-e6b5-d84e167ef4d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam 0.001\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n",
      "Epoch: 1, Training Loss: 0.01249655064514705, Validation Loss: 0.010912781151135762\n",
      "Epoch: 2, Training Loss: 0.010905010523114886, Validation Loss: 0.010864249404271444\n",
      "Epoch: 3, Training Loss: 0.010692979982921055, Validation Loss: 0.010876572998364767\n",
      "Epoch: 4, Training Loss: 0.010402227316583907, Validation Loss: 0.010953509783744812\n",
      "Epoch: 5, Training Loss: 0.009981107558522906, Validation Loss: 0.011258458876609803\n",
      "Epoch: 6, Training Loss: 0.009486561390331813, Validation Loss: 0.012215459394454956\n",
      "Epoch: 7, Training Loss: 0.009009210654667445, Validation Loss: 0.013245729919274647\n",
      "Accuracy: 0.5451726342710997, Precision: 0.5399968135540874, Recall: 0.529135998201002, F1: 0.5072153215771382\n",
      "Adam 0.01\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.48 µs\n",
      "Epoch: 1, Training Loss: 0.011893706846237183, Validation Loss: 0.010997549804051716\n",
      "Epoch: 2, Training Loss: 0.010944170662334987, Validation Loss: 0.010953066444396973\n",
      "Epoch: 3, Training Loss: 0.010829032669748578, Validation Loss: 0.011038777049382527\n",
      "Epoch: 4, Training Loss: 0.010728025712285724, Validation Loss: 0.011211437749862671\n",
      "Epoch: 5, Training Loss: 0.01060314667565482, Validation Loss: 0.011610858058929443\n",
      "Epoch: 6, Training Loss: 0.010624542212486267, Validation Loss: 0.011403175187110901\n",
      "Epoch: 7, Training Loss: 0.010472566706793649, Validation Loss: 0.011464922785758972\n",
      "Accuracy: 0.5291799872122762, Precision: 0.5266768691029691, Recall: 0.5133392875581072, F1: 0.4948698631828204\n",
      "Adam 0.0001\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n",
      "Epoch: 1, Training Loss: 0.020732571377073017, Validation Loss: 0.011115325117111207\n",
      "Epoch: 2, Training Loss: 0.010873919456345695, Validation Loss: 0.01108365916411082\n",
      "Epoch: 3, Training Loss: 0.010857592293194362, Validation Loss: 0.011072027230262757\n",
      "Epoch: 4, Training Loss: 0.010845107177325657, Validation Loss: 0.011054033287366231\n",
      "Epoch: 5, Training Loss: 0.010818009178979055, Validation Loss: 0.011034096511205037\n",
      "Epoch: 6, Training Loss: 0.010758952951431274, Validation Loss: 0.011039970016479493\n",
      "Epoch: 7, Training Loss: 0.010666681960650853, Validation Loss: 0.011030364418029785\n",
      "Accuracy: 0.5249840153452686, Precision: 0.5448126621847237, Recall: 0.5103704657036938, F1: 0.4283495919900585\n",
      "RMSprop 0.001\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.77 µs\n",
      "Epoch: 1, Training Loss: 0.011593072400774274, Validation Loss: 0.011765062514940897\n",
      "Epoch: 2, Training Loss: 0.010739772817066737, Validation Loss: 0.011080891633033753\n",
      "Epoch: 3, Training Loss: 0.010167441274438586, Validation Loss: 0.012359865013758341\n",
      "Epoch: 4, Training Loss: 0.009440433597564697, Validation Loss: 0.013642982943852742\n",
      "Epoch: 5, Training Loss: 0.008795872156960623, Validation Loss: 0.014999117946624755\n",
      "Epoch: 6, Training Loss: 0.008269280908788954, Validation Loss: 0.01621715000470479\n",
      "Epoch: 7, Training Loss: 0.007824537320647921, Validation Loss: 0.016214197715123495\n",
      "Accuracy: 0.5460837595907929, Precision: 0.5374358083622889, Recall: 0.529423715779297, F1: 0.5141837116428083\n",
      "RMSprop 0.01\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.48 µs\n",
      "Epoch: 1, Training Loss: 0.01720826315198626, Validation Loss: 0.012719235444068909\n",
      "Epoch: 2, Training Loss: 0.011892742010525295, Validation Loss: 0.011481579995155335\n",
      "Epoch: 3, Training Loss: 0.01137468796457563, Validation Loss: 0.011530587013562521\n",
      "Epoch: 4, Training Loss: 0.011245886152131217, Validation Loss: 0.011497624413172404\n",
      "Epoch: 5, Training Loss: 0.011060733774730137, Validation Loss: 0.011075237647692363\n",
      "Epoch: 6, Training Loss: 0.010952269325937543, Validation Loss: 0.011370542375246684\n",
      "Epoch: 7, Training Loss: 0.010845805522373745, Validation Loss: 0.011451187618573506\n",
      "Accuracy: 0.5099184782608696, Precision: 0.510100106584749, Recall: 0.49713600213241116, F1: 0.4592834745633495\n",
      "RMSprop 0.0001\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "Epoch: 1, Training Loss: 0.012846995319638933, Validation Loss: 0.010978156479199728\n",
      "Epoch: 2, Training Loss: 0.010868037067140852, Validation Loss: 0.010962121518452963\n",
      "Epoch: 3, Training Loss: 0.010848333328110832, Validation Loss: 0.010926707100868224\n",
      "Epoch: 4, Training Loss: 0.010771659367425102, Validation Loss: 0.010867174331347148\n",
      "Epoch: 5, Training Loss: 0.010657591676712036, Validation Loss: 0.01087099879582723\n",
      "Epoch: 6, Training Loss: 0.01056527648653303, Validation Loss: 0.010871669125556946\n",
      "Epoch: 7, Training Loss: 0.01046550736427307, Validation Loss: 0.010880204860369365\n",
      "Accuracy: 0.549704283887468, Precision: 0.5426581410961966, Recall: 0.5313756947017134, F1: 0.5116693353082806\n",
      "SGD 0.001\n",
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 5.48 µs\n",
      "Epoch: 1, Training Loss: 0.06326684269223895, Validation Loss: 0.06184392108917236\n",
      "Epoch: 2, Training Loss: 0.059755014051709854, Validation Loss: 0.058169762516021725\n",
      "Epoch: 3, Training Loss: 0.05585789087840489, Validation Loss: 0.05395597384770711\n",
      "Epoch: 4, Training Loss: 0.051249818011692594, Validation Loss: 0.04883878914515177\n",
      "Epoch: 5, Training Loss: 0.04553825547354562, Validation Loss: 0.04240775594711304\n",
      "Epoch: 6, Training Loss: 0.03838373178754534, Validation Loss: 0.03447923142115275\n",
      "Epoch: 7, Training Loss: 0.030059523378099712, Validation Loss: 0.025988244024912517\n",
      "Accuracy: 0.5060501918158568, Precision: 0.5207885514274183, Recall: 0.49177673477898615, F1: 0.3990871871090311\n",
      "SGD 0.01\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n",
      "Epoch: 1, Training Loss: 0.039816525149345396, Validation Loss: 0.013854469545682271\n",
      "Epoch: 2, Training Loss: 0.01181910823413304, Validation Loss: 0.011343749578793843\n",
      "Epoch: 3, Training Loss: 0.011086436057090759, Validation Loss: 0.011145756610234578\n",
      "Epoch: 4, Training Loss: 0.01097039234638214, Validation Loss: 0.01108224827448527\n",
      "Epoch: 5, Training Loss: 0.0109246440580913, Validation Loss: 0.011052175903320312\n",
      "Epoch: 6, Training Loss: 0.010900502262796674, Validation Loss: 0.011034961191813152\n",
      "Epoch: 7, Training Loss: 0.010885657576152256, Validation Loss: 0.011023916745185852\n",
      "Accuracy: 0.5000239769820972, Precision: 0.2653572570332481, Recall: 0.5051150895140665, F1: 0.3347648775282092\n",
      "SGD 0.0001\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n",
      "Epoch: 1, Training Loss: 0.0651710298538208, Validation Loss: 0.06531858901977539\n",
      "Epoch: 2, Training Loss: 0.0648324736731393, Validation Loss: 0.06497859929402669\n",
      "Epoch: 3, Training Loss: 0.06449415133340018, Validation Loss: 0.06463871281941731\n",
      "Epoch: 4, Training Loss: 0.06415580046517508, Validation Loss: 0.06429866822560629\n",
      "Epoch: 5, Training Loss: 0.06381715959821428, Validation Loss: 0.06395819981892904\n",
      "Epoch: 6, Training Loss: 0.06347796404702323, Validation Loss: 0.0636170462290446\n",
      "Epoch: 7, Training Loss: 0.06313795501164028, Validation Loss: 0.06327494309743245\n",
      "Accuracy: 0.5000239769820972, Precision: 0.2653572570332481, Recall: 0.5051150895140665, F1: 0.3347648775282092\n"
     ]
    }
   ],
   "source": [
    "for optim_key, optim_func in optim_dict.items():\n",
    "  # for opt_key, opt in opt_dict.items():\n",
    "    for lr_key, lr in lr_dict.items():\n",
    "    # for eps_key, eps in eps_dict.items():\n",
    "      em_sz = 200\n",
    "      nh = 300\n",
    "      v_size = len(TEXT.vocab)\n",
    "      model = RNNBaseline(nh, em_sz, v_size)\n",
    "      model.cuda()      \n",
    "      opt = optim_func(model.parameters(), lr = lr)\n",
    "        \n",
    "      print(optim_key, lr_key)\n",
    "      train_model(model, epochs, loss_func, opt) #, loss_key, eps_key, lr_key)\n",
    "      for epoch in range(1, epochs + 1):\n",
    "        acc = 0.0\n",
    "        pre = 0.0\n",
    "        rec = 0.0\n",
    "        f1 = 0.0\n",
    "        model.eval()\n",
    "        for batch in test_iter:\n",
    "          x = batch.text\n",
    "          y = batch.label\n",
    "          l = y.detach().cpu().numpy()\n",
    "          preds = model(x).detach().cpu().numpy().argmax(axis=1)\n",
    "          acc += accuracy_score(l, preds)\n",
    "          pre += precision_score(l, preds, average = 'macro')\n",
    "          rec += recall_score(l, preds, average = 'macro')\n",
    "          f1 += f1_score(l, preds, average = 'macro')\n",
    "      i = len(test_iter)\n",
    "      print(f'Accuracy: {acc/i}, Precision: {pre/i}, Recall: {rec/i}, F1: {f1/i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-DPtJYXCpHm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Практическия_реализация_NLP.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
